\subsection{Experimental Setup}
In our experiment, we use the Chinese part of ConceptNet which we collect by ChickenPTT~\cite{Kuo:HCOMP09} until May, 19, 2015. There are totally 713139 assertions and 223871 concepts. 

ANEW is used as our seeds. There are 1034 English words in ANEW. After our translation and expansion, these English words generate 27842 Chinese phrases. There are 3047 of them existing in ConceptNet. Among these 3047 concepts, there are 1654 positive, 1042 negative, 351 neutral according to sentiment values in ANEW. (After linear normalization, we define neutral using $\pm0.125$ as threshold.)

\subsection{Experiment 1: Propagation on the Same Topic}
This experiment aims to evaluate whether propagating on each topic separately is better than the original method which decides sentiment values from all neighbors together disregarding topics.

\subsubsection{Test Data}
From 3047 seed concepts, we sample 10\% of them to evaluate our propagation result and use the remaining 90\% as propagation seeds. To make the distributions of 10\% test data and the 90\% training data similar, we sample test data according to the proportions of positive, neutral and negative concepts. Therefore, in the test set there are totally 304 concepts where 165 of them are positive, 104 of them are negative and 35 of them are neutral.

\subsubsection{Evaluation Metrics}
We use point-wise accuracy and pair-wise accuracy to evaluate. Point-wise accuracy measures the proportion of concepts whose predicted polarities are same as their ground-truth to all test concepts. Pair-wise accuracy measures the proportion of pairs of concepts which have the same relative relation as ground-truth to all pairs.

\subsubsection{Post-Processing}
In our topic-aware results, each concept has different sentiment values on different topics. That is, each concept is associated with multiple sentiment values. To compare with original propagation, we aggregate values on different topics into one value. We compute a weighted arithmetic mean over sentiments on different topics. The weight of sentiment on topic $z$ is the proportion of valid neighbors belonging to $z$ among all valid neighbors.

\subsubsection{Results and Discussion}
The results of topic-insensitive and topic-aware propagation are discussed here. The former is the baseline, and we compare it with our topic-aware propagations with different topic numbers.

The results of point-wise and pair-wise accuracy are shown in Fig~\ref{fig:exp1_iter_pointAcc} and Fig~\ref{fig:exp1_iter_pairAcc} respectively. We can see that the performance of baseline become worse when propagating sentiments to more concepts. In contrast, topic-aware propagations with topic number $>2$ have higher and more stable point-wise accuracy and pairwise accuracy as iteration goes.

Here we investigate topic number $<=10$. Larger topic number causes topic layers of Chinese ConceptNet sparse and makes sentiment propagation fail. We found generalize contextual information into $<=10$ topics performs better.

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{fig/exp1_iterations/pointAcc.jpg}
\caption{Point-wise accuracy of all test concepts for the first 5 iterations.}
\label{fig:exp1_iter_pointAcc}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{fig/exp1_iterations/pairAcc.jpg}
\caption{Pairwise accuracy of all test concepts for the first 5 iterations.}
\label{fig:exp1_iter_pairAcc}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{fig/exp1_iterations/pointAcc_propagated.jpg}
\caption{Point-wise accuracy of test concepts propagated for the first 5 iterations.}
\label{fig:exp1_iter_pointAcc_known}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[width=3in]{fig/exp1_iterations/pairAcc_propagated.jpg}
\caption{Pairwise accuracy of all test concepts propagated for the first 5 iterations.}
\label{fig:exp1_iter_pairAcc_known}
\end{figure}

Some of test concepts didn't get sentiment values when propagation and are considered as neutral in our system. We exclude these concepts and investigate the performance of the remaining concepts for each iteration in Fig~\ref{fig:exp1_iter_pointAcc_known} and Fig~\ref{fig:exp1_iter_pairAcc_known}. The point-wise accuracy and pairwise accuracy reach 0.7. Except the topic number $=2$ one, topic-aware ones have higher accuracies.


%===========================
\subsection{Experiment 2: Polarity Classification for Posts of Microblog}
This experiment aims to investigate the effect of using topic-aware sentiments of concepts to predict polarities of texts. 

\subsubsection{Test Data}
The benchmark dataset is from the Chinese Microblog Sentiment Analysis Evaluation (CMSAE) task in the conference on Natural Language Processing \& Chinese Computing (NLP \& CC) 2013 \footnote{\url{http://tcci.ccf.org.cn/conference/2013/pages/page04_dg.html}}.

The original task in NLP \& CC is to recognize the emotion types of Chinese microblog texts collected from Sina Weibo. The seven emotion types are: {\it anger}, {\it disgust}, {\it fear}, {\it happiness}, {\it like}, {\it sadness}, and {\it surprise}. If the text has no emotion, it is labelled as {\it none}. The test dataset contains 10000 microblog texts and 32185 sentences. Each text is labelled a primary emotion type and a possible secondary emotion type. 

Because our system is designed for recognizing the polarity, we reduce the emotion types into polarities. The emotion types {\it anger}, {\it disgust}, {\it fear} and {\it sadness} are mapped to {\it negative}. The emotion types {\it happiness} and {\it like} are mapped to {\it positive}. We map {\it surprise} and {\it none} to {\it neutral}. For each text, we use the primary emotion as its label. We use the secondary emotion as its label only when the primary emotion is {\it neutral}. For example, if a text is labelled {\it surprise} (primary) and {\it disgust} (secondary), its polarity is {\it negative}. The distribution of microblog texts is shown in TABLE~\ref{table:microblogDist}.

\begin{table}[!t]
\centering
\caption{Distribution of Chinese microblog texts in the test set}
\label{table:microblogDist}
\begin{tabular}{|l|l|l|l|}
\hline
positive & neutral & negative & total \\ \hline
2235 & 1913 & 5852 & 10000  \\ \hline
\end{tabular}
\end{table} 

\subsubsection{Evaluation Metrics}
Following the task in NLP \& CC, we use the macro average and micro average on precision, recall and F-measure to evaluate.

\begin{equation}
\label{eq:macroPrec}
\text{Macro\_Precision} = \frac{1}{3} \sum_{i} \frac{\#system\_correct(polarity=i)}{\#system\_proposed(polarity=i)}
\end{equation}

\begin{equation}
\label{eq:macroRecall}
\text{Macro\_Recall} = \frac{1}{3} \sum_{i} \frac{\#system\_correct(polarity=i)}{\#gold\_standard(polarity=i)}
\end{equation}

\begin{equation}
\label{eq:macroF}
\text{Macro\_F-measure} = \frac{2 \times Macro\_Precision \times Macro\_Recall}{Macro\_Precision+Macro\_Recall}
\end{equation}

\begin{equation}
\label{eq:microPrec}
\text{Micro\_Precision} = \frac{\sum_{i}\#system\_correct(polarity=i)}{\sum_{i}\#system\_proposed(polarity=i)}
\end{equation}

\begin{equation}
\label{eq:microRecall}
\text{Micro\_Recall} = \frac{\sum_{i}\#system\_correct(polarity=i)}{\sum_{i}\#gold\_standard(polarity=i)}
\end{equation}

\begin{equation}
\label{eq:microF}
\text{Micro\_F-measure} = \frac{2 \times Micro\_Precision \times Micro\_Recall}{Micro\_Precision+Micro\_Recall}
\end{equation}

\subsubsection{Lexicon Based Polarity Prediction}
We predict the polarity for each post by a basic lexicon based approach. We use Chinese text segmentation tool jieba~\footnote{https://pypi.python.org/pypi/jieba/0.37} to sentences in the post and find words or phrases appearing in ConceptNet. 

Here we compare the performance of concepts' sentiments generated by methods in TABLE~\ref{table:comparePredict}. For methods which associates each concept with one sentiment value, we compare both performance of iteration 2 and 5. The iteration 2 is more conservative while the iteration 5 is more aggressive. In M2, we use the concepts in all sentences in the post as co-occurring concepts, and the system will return the topic and sentiment value for each concept. Because we observe that topic-aware propagation is more effective as iteration goes, M2 use the result when iteration $=5$. 

\begin{table}[!t]
\centering
\caption{Methods to predict polarities of dialogues}
\label{table:comparePredict}
\begin{tabular}{|l|l|}
\hline
Method    & Explanation  \\ \hline
Baseline-iteration2 or 5 & \makecell[l]{In-link propagation with relation rules} \\ \hline
M1-iteration2 or 5 & \makecell[l]{Baseline + Topic based propagation \\ $\rightarrow$ each concept associated with \\ one sentiment value} \\ \hline
M2 & \makecell[l]{Baseline + Topic based propagation \\ $\rightarrow$ each concept has different \\ sentiment values on different topics} \\ \hline
\end{tabular}
\end{table}

After knowing the sentiment values of concepts in posts, we define commonly used negation words. Then we use negation words to negate the sentiment values of those concepts behind it. Finally, all sentiment values are summed up, and we use its sign as the polarity of this post. ($\pm 0.125$ as the threshold of neutral) 

\subsubsection{Results and Discussion}
The performance of M2 using different topic number is shown in TABLE~\ref{table:microblogM2}. M2 using more topics performs better. TABLE~\ref{table:microblogM1_2} and TABLE~\ref{table:microblogM1_5} are results of using M1-iteration2 and M1-iteration5 respectively. TABLE~\ref{table:microblogbase} shows the results of baseline. We can see that the results of M2 are generally better than them. This shows the effectiveness of our topic-aware approach. 

\begin{table}[!t]
\centering
\caption{Results of using M2}
\label{table:microblogM2}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{|l|}{macro-average} & \multicolumn{3}{|l|}{micro-average} \\ \cline{2-7}
& prec & recall & F score & prec & recall & F score \\ \hline
2 topics & 0.342 & 0.371 & 0.356 & 0.469 & 0.469 & 0.469 \\ \hline
3 topics & 0.356 & 0.345 & 0.350 & 0.462 & 0.462 & 0.462 \\ \hline
4 topics & 0.363 & 0.365 & 0.364 & 0.421 & 0.421 & 0.421 \\ \hline
5 topics & 0.399 & 0.392 & 0.396 & 0.487 & 0.487 & 0.487 \\ \hline
6 topics & 0.390 & 0.390 & 0.390 & 0.472 & 0.472 & 0.472 \\ \hline
7 topics & 0.420 & 0.404 & 0.412 & 0.516 & 0.516 & 0.516 \\ \hline
8 topics & 0.420 & 0.404 & 0.411 & 0.527 & 0.527 & 0.527 \\ \hline
9 topics & 0.427 & 0.402 & 0.414 & 0.534 & 0.534 & 0.534 \\ \hline
10 topics & 0.433 & 0.406 & 0.419 & 0.524 & 0.524 & 0.524 \\ \hline
\end{tabular}
\end{table}

\begin{table}[!t]
\centering
\caption{Results of using M1-iteration2}
\label{table:microblogM1_2}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{|l|}{macro-average} & \multicolumn{3}{|l|}{micro-average} \\ \cline{2-7}
& prec & recall & F score & prec & recall & F score \\ \hline
2 topics & 0.377 & 0.367 & 0.372 & 0.251 & 0.251 & 0.251 \\ \hline
3 topics & 0.367 & 0.371 & 0.369 & 0.254 & 0.254 & 0.254 \\ \hline
4 topics & 0.388 & 0.377 & 0.382 & 0.257 & 0.257 & 0.257 \\ \hline
5 topics & 0.365 & 0.378 & 0.372 & 0.258 & 0.258 & 0.258 \\ \hline
6 topics & 0.378 & 0.375 & 0.377 & 0.255 & 0.255 & 0.255 \\ \hline
7 topics & 0.364 & 0.378 & 0.370 & 0.257 & 0.257 & 0.257 \\ \hline
8 topics & 0.366 & 0.381 & 0.373 & 0.261 & 0.261 & 0.261 \\ \hline
9 topics & 0.376 & 0.384 & 0.380 & 0.263 & 0.263 & 0.263 \\ \hline
10 topics & 0.380 & 0.380 & 0.380 & 0.260 & 0.260 & 0.260 \\ \hline
\end{tabular}
\end{table} 

\begin{table}[!t]
\centering
\caption{Results of using M1-iteration5}
\label{table:microblogM1_5}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{|l|}{macro-average} & \multicolumn{3}{|l|}{micro-average} \\ \cline{2-7}
& prec & recall & F score & prec & recall & F score \\ \hline
2 topics & 0.365 & 0.352 & 0.358 & 0.242 & 0.242 & 0.242 \\ \hline
3 topics & 0.374 & 0.369 & 0.372 & 0.256 & 0.256 & 0.256 \\ \hline
4 topics & 0.361 & 0.373 & 0.367 & 0.253 & 0.253 & 0.253 \\ \hline
5 topics & 0.372 & 0.382 & 0.377 & 0.260 & 0.260 & 0.260 \\ \hline
6 topics & 0.366 & 0.377 & 0.371 & 0.257 & 0.257 & 0.257 \\ \hline
7 topics & 0.369 & 0.381 & 0.375 & 0.261 & 0.261 & 0.261 \\ \hline
8 topics & 0.388 & 0.386 & 0.387 & 0.265 & 0.265 & 0.265 \\ \hline
9 topics & 0.383 & 0.389 & 0.386 & 0.266 & 0.266 & 0.266 \\ \hline
10 topics & 0.368 & 0.377 & 0.372 & 0.258 & 0.258 & 0.258 \\ \hline
\end{tabular}
\end{table}

\begin{table}[!t]
\centering
\caption{Results of using Baseline-iteration2 and 5}
\label{table:microblogbase}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\multirow{2}{*}{Method} & \multicolumn{3}{|l|}{macro-average} & \multicolumn{3}{|l|}{micro-average} \\ \cline{2-7}
& prec & recall & F score & prec & recall & F score \\ \hline
iteration2 & 0.386 & 0.358 & 0.372 & 0.244 & 0.244 & 0.244 \\ \hline
iteration5 & 0.355 & 0.349 & 0.352 & 0.238 & 0.238 & 0.238 \\ \hline
\end{tabular}
\end{table}
